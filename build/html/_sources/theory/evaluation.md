# Evaluation

To properly prove that the generative models that are used in the Argus system are indeed real-like and a useful tool for evaluation of ML models they need to be thoroughly evaluated and compared to other models. The synthetic images should be evaluated both qualitatively and quantitatively to ensure that they are indeed real-like and any models that use those images for evaluation can be expected to perform similarly in the real world. This is especially important for safety-critical systems in ADAS and autonomous vehicles. If this cannot be proved with statistical certainty, then the results of the evaluations will not transfer well to the real world(Sim-to-Real gap) and the use of this system for validation is then worthless. The evaluation follows the procedure outlined in [Multi-weather city: Adverse weather stacking for autonomous driving by Musat et al](https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Musat_Multi-Weather_City_Adverse_Weather_Stacking_for_Autonomous_Driving_ICCVW_2021_paper.pdf) as it seems robust enough.

## Qualitative Evalution of Generative Models
To evaluate the synthetical augmentation of datasets the generated output from the adverse weather GAN stack can be evaluated with the [Frechet Inception Distance](https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf) and [Inception Score](https://papers.nips.cc/paper/2016/hash/8a3363abe792db2d8761d6403605aeb7-Abstract.html). These two metrics give an estimate of the image quality and its suitability for evaluation in OD and segmentation tasks. A low FID score and a high IS would represent a good synthetic image. Images that don't fulfill this criteria can be marked for further evaluation as images of interest that can guide future improvements in the generative model.

## Quantitative Evaluation of Generative Models

To quantitatively evaluate the synthetically augmented-dataset this dataset is then used to train multiple reference Evaluation ML models that perform specific ADAS tasks such as OD and segmentation. Then, the model is tested in Out-of-Distribution samples(for example a different dataset) and its performance is contrasted versus the real-only trained model. By evaluating the performance of the downstream tasks the validity of the synthetic augmentation can be confirmed if it helps improve the evaluation metrics on the test dataset w.r.t the real-trained. If both the quantitative and qualitative metrics show correlation this doubly confirms the validity of the generative model at correctly learning the domain shift and thus implying the synthetic images are real-like (and thus representative of a real-world test). A model that performs better over multiple evaluation models(provided a diverse and balanced testing dataset with the desired adverse weather conditions) has a higher statistical certainty and can be expected to perform well on the real world. 

## Evaluation of the actual models

The main goal of Argus is to provide ML model evaluation using synthetically augmented datasets to test the performance in multiple adverse weather conditions (which is hard to accomplish in traditional datasets). For this reason the ML models need to be thoroughly evaluated with multiple metrics that accurately describe the performance of the model for the given task using the synthetically augmented test dataset. Depending on the scope of each model is the evaluation scheme that will be used (i.e. IoU for segmentation, AP for object detection, etc.). All of the ML model testing must be done using out-of-distribution images,both the synthetic as well as the source/real (or risk erroneously overperforming metrics due to a priori biases). Additionally, the synthetically generated dataset should be balanced in its creation to ensure the different adverse weather conditions are applied equally and there is no overfitting due to a mismatched oversampling of specific scenarios.